{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <font color=\"darkgreen\">Data Preperation Exercises</font>\n",
    "\n",
    "The end result of this exercise should be a file named `prepare.py` that defines the requested functions.\n",
    "\n",
    "In this exercise we will be defining some functions to prepare textual data. These functions should apply equally well to both the codeup blog articles and the news articles that were previously acquired.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import unicodedata\n",
    "import re\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import acquire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color =\"blue\"> 1. Define a function named `basic_clean`. It should take in a string and apply some basic text cleaning to it:\n",
    "* Lowercase everything\n",
    "* Normalize unicode characters\n",
    "* Replace anything that is not a letter, number, whitespace or a single quote.</font>\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Paul Erdős and George Pólya are influential Hungarian mathematicians who contributed a lot to the field. Erdős's name contains the Hungarian letter 'ő' ('o' with double acute accent), but is often incorrectly written as Erdos or Erdös either by mistake or out of typographical necessity\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original = \"Paul Erdős and George Pólya are influential Hungarian mathematicians who contributed a lot to the field. Erdős's name contains the Hungarian letter 'ő' ('o' with double acute accent), but is often incorrectly written as Erdos or Erdös either by mistake or out of typographical necessity\"\n",
    "original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paul erdős and george pólya are influential hungarian mathematicians who contributed a lot to the field. erdős's name contains the hungarian letter 'ő' ('o' with double acute accent), but is often incorrectly written as erdos or erdös either by mistake or out of typographical necessity\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lowercase all letters in the text\n",
    "article =  original.lower()\n",
    "article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paul erdos and george polya are influential hungarian mathematicians who contributed a lot to the field. erdos's name contains the hungarian letter 'o' ('o' with double acute accent), but is often incorrectly written as erdos or erdos either by mistake or out of typographical necessity\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove inconsistencies in unicode character encoding.\n",
    "# encode the strings into ASCII bytestrings (ignore non-ASCII characters)\n",
    "# decode the bytestring into (Unicode) string\n",
    "\n",
    "article = unicodedata.normalize('NFKD', article)\\\n",
    ".encode('ascii', 'ignore')\\\n",
    ".decode('utf-8', 'ignore')\n",
    "\n",
    "\n",
    "article\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paul erdos and george polya are influential hungarian mathematicians who contributed a lot to the field erdos's name contains the hungarian letter 'o' 'o' with double acute accent but is often incorrectly written as erdos or erdos either by mistake or out of typographical necessity\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove anything that is not a through z, a number, a single quote, or whitespace\n",
    "\n",
    "article = re.sub(r\"[^a-z0-9'\\s]\", '', article)\n",
    "article\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_clean(string):\n",
    "    '''\n",
    "    This function takes in a string and\n",
    "    returns the string normalized.\n",
    "    '''\n",
    "    string = unicodedata.normalize('NFKC', string)\\\n",
    "             .encode('ascii', 'ignore')\\\n",
    "             .decode('utf-8', 'ignore')\n",
    "    string = re.sub(r'[^\\w\\s]', '', string).lower()\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paul erdos and george polya are influential hungarian mathematicians who contributed a lot to the field erdoss name contains the hungarian letter o o with double acute accent but is often incorrectly written as erdos or erdos either by mistake or out of typographical necessity'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_clean(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color =\"blue\"> 2. Define a function named `tokenize`. It should take in a string and tokenize all the words in the string.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paul erdos and george polya are influential hungarian mathematicians who contributed a lot to the field erdos ' s name contains the hungarian letter ' o ' ' o ' with double acute accent but is often incorrectly written as erdos or erdos either by mistake or out of typographical necessity\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the tokenizer\n",
    "tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "\n",
    "# Use the tokenizer\n",
    "tokenizer.tokenize(article, return_str = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize function\n",
    "def tokenize(string):\n",
    "    '''\n",
    "    This function takes in a string and\n",
    "    returns a tokenized string.\n",
    "    '''\n",
    "    # Create tokenizer.\n",
    "    tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "    \n",
    "    # Use tokenizer\n",
    "    string = tokenizer.tokenize(string, return_str=True)\n",
    "    \n",
    "    return string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paul erdos and george polya are influential hungarian mathematicians who contributed a lot to the field erdos ' s name contains the hungarian letter ' o ' ' o ' with double acute accent but is often incorrectly written as erdos or erdos either by mistake or out of typographical necessity\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color =\"blue\">3. Define a function named `stem`. It should accept some text and return the text after applying stemming to all the words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create porter stemmer.\n",
    "\n",
    "ps = nltk.porter.PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paul',\n",
       " 'erdo',\n",
       " 'and',\n",
       " 'georg',\n",
       " 'polya',\n",
       " 'are',\n",
       " 'influenti',\n",
       " 'hungarian',\n",
       " 'mathematician',\n",
       " 'who']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply the stemmer to each word in our string.\n",
    "\n",
    "stems =[ps.stem(word) for word in article.split()]\n",
    "stems[:10] \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paul erdo and georg polya are influenti hungarian mathematician who contribut a lot to the field erdos' name contain the hungarian letter 'o' 'o' with doubl acut accent but is often incorrectli written as erdo or erdo either by mistak or out of typograph necess\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join our lists of words into a string again\n",
    "\n",
    "article_stemmed =  ' '.join(stems)\n",
    "article_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stem function\n",
    "def stem(string):\n",
    "    '''\n",
    "    This function takes in a string and\n",
    "    returns a string with words stemmed.\n",
    "    '''\n",
    "    # Create porter stemmer.\n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    \n",
    "    # Use the stemmer to stem each word in the list of words we created by using split.\n",
    "    stems = [ps.stem(word) for word in string.split()]\n",
    "    \n",
    "    # Join our lists of words into a string again and assign to a variable.\n",
    "    string = ' '.join(stems)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paul erdo and georg polya are influenti hungarian mathematician who contribut a lot to the field erdos' name contain the hungarian letter 'o' 'o' with doubl acut accent but is often incorrectli written as erdo or erdo either by mistak or out of typograph necess\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stem(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color =\"blue\"> 4. Define a function named `lemmatize`. It should accept some text and return the text after applying lemmatization to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/brandonbryant/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the first time.\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Lemmatizer.\n",
    "\n",
    "wnl = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'influence'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check lemmatizer. It works.\n",
    "\n",
    "wnl.lemmatize('influence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paul',\n",
       " 'erdos',\n",
       " 'and',\n",
       " 'george',\n",
       " 'polya',\n",
       " 'are',\n",
       " 'influential',\n",
       " 'hungarian',\n",
       " 'mathematician',\n",
       " 'who']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the lemmatizer on each word in the list of words we created by using split.\n",
    "\n",
    "lemmas = [wnl.lemmatize(word) for word in article.split()]\n",
    "lemmas[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paul erdos and george polya are influential hungarian mathematician who contributed a lot to the field erdos's name contains the hungarian letter 'o' 'o' with double acute accent but is often incorrectly written a erdos or erdos either by mistake or out of typographical necessity\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join our list of words into a string again; assign to a variable to save changes.\n",
    "\n",
    "article_lemmatized = ' '.join(lemmas)\n",
    "article_lemmatized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lemmatize fucntion\n",
    "def lemmatize(string):\n",
    "    '''\n",
    "    This function takes in string for and\n",
    "    returns a string with words lemmatized.\n",
    "    '''\n",
    "    # Create the lemmatizer.\n",
    "    wnl = nltk.stem.WordNetLemmatizer()\n",
    "    \n",
    "    # Use the lemmatizer on each word in the list of words we created by using split.\n",
    "    lemmas = [wnl.lemmatize(word) for word in string.split()]\n",
    "    \n",
    "    # Join our list of words into a string again and assign to a variable.\n",
    "    string = ' '.join(lemmas)\n",
    "    \n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paul erdos and george polya are influential hungarian mathematician who contributed a lot to the field erdos's name contains the hungarian letter 'o' 'o' with double acute accent but is often incorrectly written a erdos or erdos either by mistake or out of typographical necessity\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatize(article)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color =\"blue\"> 5. Define a function named `remove_stopwords`. It should accept some text and return the text after removing all the stopwords.</font>\n",
    "- This function should define two optional parameters, `extra_words` and `exclude_words`. These parameters should define any additional stop words to include, and any words that we don't want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/brandonbryant/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# standard English language stopwords list from nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "stopword_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopword_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopword_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paul',\n",
       " 'erdos',\n",
       " 'and',\n",
       " 'george',\n",
       " 'polya',\n",
       " 'are',\n",
       " 'influential',\n",
       " 'hungarian',\n",
       " 'mathematician',\n",
       " 'who']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split words in lemmatized article.\n",
    "\n",
    "words = article_lemmatized.split()\n",
    "words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['paul',\n",
       " 'erdos',\n",
       " 'george',\n",
       " 'polya',\n",
       " 'influential',\n",
       " 'hungarian',\n",
       " 'mathematician',\n",
       " 'contributed',\n",
       " 'lot',\n",
       " 'field']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a list of words from my string with stopwords removed and assign to variable.\n",
    "\n",
    "filtered_words = [word for word in words if word not in stopword_list]\n",
    "filtered_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paul erdos george polya influential hungarian mathematician contributed lot field erdos's name contains hungarian letter 'o' 'o' double acute accent often incorrectly written erdos erdos either mistake typographical necessity\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join words in the list back into strings; assign to a variable to keep changes.\n",
    "\n",
    "article_without_stopwords = ' '.join(filtered_words)\n",
    "article_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_stopwords(string, extra_words=[], exclude_words=[]):\n",
    "    '''\n",
    "    This function takes in a string, optional extra_words and exclude_words parameters\n",
    "    with default empty lists and returns a string.\n",
    "    '''\n",
    "    # Create stopword_list.\n",
    "    stopword_list = stopwords.words('english')\n",
    "    \n",
    "    # Remove 'exclude_words' from stopword_list to keep these in my text.\n",
    "    stopword_list = set(stopword_list) - set(exclude_words)\n",
    "\n",
    "    # Add in 'extra_words' to stopword_list.\n",
    "    stopword_list = stopword_list.union(set(extra_words))\n",
    "    \n",
    "    # Split words in string.\n",
    "    words = string.split()\n",
    "    \n",
    "    # Create a list of words from my string with stopwords removed and assign to variable.\n",
    "    filtered_words = [word for word in words if word not in stopword_list]\n",
    "    \n",
    "    # Join words in the list back into strings and assign to a variable.\n",
    "    string_without_stopwords = ' '.join(filtered_words)\n",
    "    \n",
    "    return string_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"paul erdos george polya influential hungarian mathematicians contributed lot field erdos's name contains hungarian letter 'o' 'o' double acute accent often incorrectly written erdos erdos either mistake typographical necessity\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "string1 = \"hbsshabyh eabah hbwbwkh jknrghrjbgurb ugwbigbahuiwgeag ugauywevfluweibf ueVHFwvuf UEBFYWliY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hbsshabyh eabah hbwbwkh jknrghrjbgurb ugwbigbahuiwgeag ugauywevfluweibf ueVHFwvuf UEBFYWliY'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "remove_stopwords(string1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color =\"blue\"> 6. Use your data from the acquire to produce a dataframe of the news articles. Name the dataframe 'news_df'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define categories\n",
    "categories = [\"business\", \"sports\", \"technology\", \"entertainment\"]\n",
    "\n",
    "# use get_all_new_article function from acquire.py file \n",
    "\n",
    "news_df = acquire.get_all_news_articles(categories)                                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>India underestimated the coronavirus: Raghuram...</td>\n",
       "      <td>Speaking about India's second COVID-19 wave, f...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Air India pilots demand vaccination on priorit...</td>\n",
       "      <td>Indian Commercial Pilots Association (ICPA) on...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>South Korea's richest woman gets fortune worth...</td>\n",
       "      <td>South Korea’s richest woman Hong Ra-hee added ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>World's biggest jeweller says it will no longe...</td>\n",
       "      <td>Pandora, the world's biggest jeweller, has sai...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Samsung pledges ₹37 crore to India to fight CO...</td>\n",
       "      <td>Samsung has pledged $5 million (around ₹37 cro...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  India underestimated the coronavirus: Raghuram...   \n",
       "1  Air India pilots demand vaccination on priorit...   \n",
       "2  South Korea's richest woman gets fortune worth...   \n",
       "3  World's biggest jeweller says it will no longe...   \n",
       "4  Samsung pledges ₹37 crore to India to fight CO...   \n",
       "\n",
       "                                             content  category  \n",
       "0  Speaking about India's second COVID-19 wave, f...  business  \n",
       "1  Indian Commercial Pilots Association (ICPA) on...  business  \n",
       "2  South Korea’s richest woman Hong Ra-hee added ...  business  \n",
       "3  Pandora, the world's biggest jeweller, has sai...  business  \n",
       "4  Samsung has pledged $5 million (around ₹37 cro...  business  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color =\"blue\"> 7. Make another dataframe for the Codeup blog posts. Name the dataframe codeup_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>published_date</th>\n",
       "      <th>blog_image</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Codeup’s Data Science Career Accelerator is Here!</td>\n",
       "      <td>September 30, 2018</td>\n",
       "      <td>https://codeup.com/wp-content/uploads/2018/10/...</td>\n",
       "      <td>The rumors are true! The time has arrived. Cod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Science Myths</td>\n",
       "      <td>October 31, 2018</td>\n",
       "      <td>https://codeup.com/wp-content/uploads/2018/10/...</td>\n",
       "      <td>By Dimitri Antoniou and Maggie Giust\\nData Sci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Science VS Data Analytics: What’s The Dif...</td>\n",
       "      <td>October 17, 2018</td>\n",
       "      <td>https://codeup.com/wp-content/uploads/2018/10/...</td>\n",
       "      <td>By Dimitri Antoniou\\nA week ago, Codeup launch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10 Tips to Crush It at the SA Tech Job Fair</td>\n",
       "      <td>August 14, 2018</td>\n",
       "      <td>None</td>\n",
       "      <td>SA Tech Job Fair\\nThe third bi-annual San Anto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Competitor Bootcamps Are Closing. Is the Model...</td>\n",
       "      <td>August 14, 2018</td>\n",
       "      <td>None</td>\n",
       "      <td>Competitor Bootcamps Are Closing. Is the Model...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title      published_date  \\\n",
       "0  Codeup’s Data Science Career Accelerator is Here!  September 30, 2018   \n",
       "1                                 Data Science Myths    October 31, 2018   \n",
       "2  Data Science VS Data Analytics: What’s The Dif...    October 17, 2018   \n",
       "3        10 Tips to Crush It at the SA Tech Job Fair     August 14, 2018   \n",
       "4  Competitor Bootcamps Are Closing. Is the Model...     August 14, 2018   \n",
       "\n",
       "                                          blog_image  \\\n",
       "0  https://codeup.com/wp-content/uploads/2018/10/...   \n",
       "1  https://codeup.com/wp-content/uploads/2018/10/...   \n",
       "2  https://codeup.com/wp-content/uploads/2018/10/...   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "                                             content  \n",
       "0  The rumors are true! The time has arrived. Cod...  \n",
       "1  By Dimitri Antoniou and Maggie Giust\\nData Sci...  \n",
       "2  By Dimitri Antoniou\\nA week ago, Codeup launch...  \n",
       "3  SA Tech Job Fair\\nThe third bi-annual San Anto...  \n",
       "4  Competitor Bootcamps Are Closing. Is the Model...  "
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from acquire import acquire_codeup_blog\n",
    "codeup_df = acquire_codeup_blog()\n",
    "codeup_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color =\"blue\"> 8. For each dataframe, produce the following columns:</font>\n",
    "\n",
    "   * 'title' to hold the title\n",
    "   * 'original' to hold the original article/post content\n",
    "   * 'clean' to hold the normalized and tokenized original with the stopwords removed.\n",
    "   * 'stemmed' to hold the stemmed version of the cleaned data.\n",
    "   * 'lemmatized' to hold the lemmatized version of the cleaned data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_article_data(df, column, extra_words=[], exclude_words=[]):\n",
    "    '''\n",
    "    This function take in a df and the string name for a text column with \n",
    "    option to pass lists for extra_words and exclude_words and\n",
    "    returns a df with the text article title, original text, stemmed text,\n",
    "    lemmatized text, cleaned, tokenized, & lemmatized text with stopwords removed.\n",
    "    '''\n",
    "    df['clean'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\n",
    "    \n",
    "    df['stemmed'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(stem)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\n",
    "    \n",
    "    df['lemmatized'] = df[column].apply(basic_clean)\\\n",
    "                            .apply(tokenize)\\\n",
    "                            .apply(lemmatize)\\\n",
    "                            .apply(remove_stopwords, \n",
    "                                   extra_words=extra_words, \n",
    "                                   exclude_words=exclude_words)\n",
    "    \n",
    "    return df[['title', column,'clean', 'stemmed', 'lemmatized']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>clean</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>India underestimated the coronavirus: Raghuram...</td>\n",
       "      <td>Speaking about India's second COVID-19 wave, f...</td>\n",
       "      <td>speaking indias second covid19 wave former rbi...</td>\n",
       "      <td>speak india second covid19 wave former rbi gov...</td>\n",
       "      <td>speaking india second covid19 wave former rbi ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Air India pilots demand vaccination on priorit...</td>\n",
       "      <td>Indian Commercial Pilots Association (ICPA) on...</td>\n",
       "      <td>indian commercial pilots association icpa tues...</td>\n",
       "      <td>indian commerci pilot associ icpa tuesday said...</td>\n",
       "      <td>indian commercial pilot association icpa tuesd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>South Korea's richest woman gets fortune worth...</td>\n",
       "      <td>South Korea’s richest woman Hong Ra-hee added ...</td>\n",
       "      <td>south koreas richest woman hong rahee added an...</td>\n",
       "      <td>south korea richest woman hong rahe ad anoth 7...</td>\n",
       "      <td>south korea richest woman hong rahee added ano...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>World's biggest jeweller says it will no longe...</td>\n",
       "      <td>Pandora, the world's biggest jeweller, has sai...</td>\n",
       "      <td>pandora worlds biggest jeweller said itll stop...</td>\n",
       "      <td>pandora world biggest jewel said itll stop use...</td>\n",
       "      <td>pandora world biggest jeweller said itll stop ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Samsung pledges ₹37 crore to India to fight CO...</td>\n",
       "      <td>Samsung has pledged $5 million (around ₹37 cro...</td>\n",
       "      <td>samsung pledged 5 million around 37 crore help...</td>\n",
       "      <td>samsung pledg 5 million around 37 crore help i...</td>\n",
       "      <td>samsung pledged 5 million around 37 crore help...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  India underestimated the coronavirus: Raghuram...   \n",
       "1  Air India pilots demand vaccination on priorit...   \n",
       "2  South Korea's richest woman gets fortune worth...   \n",
       "3  World's biggest jeweller says it will no longe...   \n",
       "4  Samsung pledges ₹37 crore to India to fight CO...   \n",
       "\n",
       "                                             content  \\\n",
       "0  Speaking about India's second COVID-19 wave, f...   \n",
       "1  Indian Commercial Pilots Association (ICPA) on...   \n",
       "2  South Korea’s richest woman Hong Ra-hee added ...   \n",
       "3  Pandora, the world's biggest jeweller, has sai...   \n",
       "4  Samsung has pledged $5 million (around ₹37 cro...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  speaking indias second covid19 wave former rbi...   \n",
       "1  indian commercial pilots association icpa tues...   \n",
       "2  south koreas richest woman hong rahee added an...   \n",
       "3  pandora worlds biggest jeweller said itll stop...   \n",
       "4  samsung pledged 5 million around 37 crore help...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  speak india second covid19 wave former rbi gov...   \n",
       "1  indian commerci pilot associ icpa tuesday said...   \n",
       "2  south korea richest woman hong rahe ad anoth 7...   \n",
       "3  pandora world biggest jewel said itll stop use...   \n",
       "4  samsung pledg 5 million around 37 crore help i...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  speaking india second covid19 wave former rbi ...  \n",
       "1  indian commercial pilot association icpa tuesd...  \n",
       "2  south korea richest woman hong rahee added ano...  \n",
       "3  pandora world biggest jeweller said itll stop ...  \n",
       "4  samsung pledged 5 million around 37 crore help...  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use the function defined above for news_df's content column\n",
    "prep_article_data(news_df,'content', extra_words = ['ha'], exclude_words = ['no']).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"blue\">9. Ask yourself:</font>\n",
    "\n",
    "* If your corpus is 493KB, would you prefer to use stemmed or lemmatized text? -Lemmatized\n",
    "* If your corpus is 25MB, would you prefer to use stemmed or lemmatized text? -Personal preference\n",
    "* If your corpus is 200TB of text and you're charged by the megabyte for your hosted computational resources, would you prefer to use stemmed or lemmatized text? -Stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
